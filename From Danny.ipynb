{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfiles = glob.glob(\"C:/Users/Danny/Desktop/TEXTFILES/*.txt\")\n",
    "len(textfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(textfiles[1],encoding=\"utf8\")\n",
    "text = file.read()\n",
    "bigram = list(nltk.trigrams(nltk.word_tokenize(text)))\n",
    "print(*map(' '.join, bigram), sep=', ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = [nltk.word_tokenize(word) for word in text.split() if len(word) >2]\n",
    "bigram = list(nltk.trigrams(nltk.word_tokenize(clean)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(\" \".join(clean))\n",
    "bigram = nltk.bigrams(tokens)\n",
    "raw = ', '.join(' '.join((a, b)) for a, b in bigram)\n",
    "def remove_punct(text):\n",
    "    return re.compile(r\"[^\\d\\W]+\").findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= remove_punct(\" \".join(map(str, clean)))\n",
    "Counter(a).most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "resp = vectorizer.fit_transform([open(file, encoding=\"utf8\").read() for file in textfiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second way:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [open(file, encoding=\"utf8\").read() for file in textfiles]\n",
    "vetorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "#print (vectorizer.vocabulary_)\n",
    "vector = vectorizer.transform(text)\n",
    "print (vector.shape)\n",
    "print (type(vector))\n",
    "print (vector.toarray())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "#vectorizer.vocabulary_\n",
    "#vectorizer.idf_\n",
    "vector = vectorizer.transform([text[32]])\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = {}\n",
    "for doc in corpus_tfidf:\n",
    "    for iWord, tf_idf in doc:\n",
    "        if iWord not in topWords:\n",
    "            topWords[iWord] = 0\n",
    "\n",
    "        if tf_idf > topWords[iWord]:\n",
    "            topWords[iWord] = tf_idf\n",
    "\n",
    "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(\"%2s: %-13s %s\" % (i, dictionary[item[0]], item[1]))\n",
    "    if i == 6: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first build 2 lists of pos and neg\n",
    "positive_vocab = [\"מצוין\", \"אדיר\"]\n",
    "negative_vocab = [\"גרוע\"]\n",
    "neutral_vocab = [\"בסדר\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    "neutral_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome movie, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "language_code = \"he\"\n",
    "inp = language_code+\"wiki-latest-pages-articles.xml.bz2\"\n",
    "outp = \"wiki.{}.text\".format(language_code)\n",
    "i = 0\n",
    "\n",
    "print(\"Starting to create wiki corpus\")\n",
    "output = open(outp, 'w')\n",
    "space = \" \"\n",
    "wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n",
    "for text in wiki.get_texts():\n",
    "  article = space.join([t.decode(\"utf-8\") for t in text])\n",
    "\n",
    "  output.write(article + \"\\n\")\n",
    "  i = i + 1\n",
    "  if (i % 1000 == 0):\n",
    "    print(\"Saved \" + str(i) + \" articles\")\n",
    "\n",
    "output.close()\n",
    "print(\"Finished - Saved \" + str(i) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
