{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Regulator Text Analsyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Importing Various needed modules\n",
    "import glob, nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "textfiles = glob.glob(\"../Data/*_intro.txt\")\n",
    "len(textfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all txt files into one corpus\n",
    "#corpus = PlaintextCorpusReader(\"../Data\", \".txt\",encoding='utf-8') #All text files \n",
    "corpus = PlaintextCorpusReader(\"../Data\", \".*intro.txt\",encoding='utf-8') #All yearly reportintro text files\n",
    "#corpus = PlaintextCorpusReader(\"../Data\", \"2004.*txt\",encoding='utf-8') #All 2004 text files\n",
    "#corpus = PlaintextCorpusReader(\"../Data\", \".*y.*txt\",encoding='utf-8') #All yearly report text files\n",
    "\n",
    "all_text=corpus.raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b3d20175b211>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1999\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-b3d20175b211>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1999\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    " ({}.format(year=year) for year in range(1999,2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999y_intro.txt -- 1119\n",
      "2000y_intro.txt -- 954\n",
      "2001y_intro.txt -- 1050\n",
      "2002y_intro.txt -- 916\n",
      "2003y_intro.txt -- 849\n",
      "2004y_intro.txt -- 1164\n",
      "2005y_intro.txt -- 916\n",
      "2006y_intro.txt -- 924\n",
      "2007y_intro.txt -- 1245\n",
      "2008y_intro.txt -- 1138\n",
      "2009y_intro.txt -- 1134\n",
      "2010y_intro.txt -- 1043\n",
      "2011y_intro.txt -- 1054\n",
      "2012y_intro.txt -- 1066\n",
      "2013y_intro.txt -- 1080\n",
      "2014y_intro.txt -- 1289\n",
      "2015y_intro.txt -- 1416\n",
      "2016y_intro.txt -- 1421\n",
      "2017y_intro.txt -- 1495\n"
     ]
    }
   ],
   "source": [
    "files=corpus.fileids()\n",
    "for file in files:\n",
    "    raw = corpus.raw(file)\n",
    "    print(file,\"--\",len(nltk.word_tokenize(raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctation marks ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~') from raw text\n",
    "all_text2= all_text.translate(str.maketrans(\"\",\"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words shorter than 2 letters and converting back to string\n",
    "clean = ' '.join([word for word in all_text2.split() if len(word) >2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing full string into a list of words\n",
    "words = nltk.word_tokenize(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTS = []\n",
    "for year in range(1999,2017):\n",
    "    TEXTS.append(locals()['text' + str(year)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119\n",
      "954\n",
      "1050\n",
      "882\n",
      "849\n",
      "1164\n",
      "916\n",
      "924\n",
      "1245\n",
      "1138\n",
      "1134\n",
      "1043\n",
      "1054\n",
      "1066\n",
      "1080\n",
      "1289\n",
      "1416\n",
      "1421\n"
     ]
    }
   ],
   "source": [
    "for text in TEXTS:\n",
    "    print(len(nltk.word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([sent for sent in sentences if \"תחרות\" in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If object is string - length is number of letters.\n",
    "# If object is list - length is number of elements in list -\n",
    "# so if object words - it's number of words.\n",
    "for i in all_text2, clean, words, sentences:\n",
    "    print(type(i))\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('הבנקים', 368),\n",
       " ('האשראי', 179),\n",
       " ('הפיקוח', 153),\n",
       " ('הבנקאית', 125),\n",
       " ('ההון', 117),\n",
       " ('המערכת', 99),\n",
       " ('בישראל', 85),\n",
       " ('בשנים', 79),\n",
       " ('השנה', 77),\n",
       " ('הבנקאות', 76)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count('בישראל')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(bigram)).count(('השנה הנסקרת'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = nltk.bigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('הפיקוח', 'הבנקים'), 95),\n",
       " (('המערכת', 'הבנקאית'), 73),\n",
       " (('בשנים', 'האחרונות'), 51),\n",
       " (('התאגידים', 'הבנקאיים'), 38),\n",
       " (('המפקח', 'הבנקים'), 37),\n",
       " (('מערכת', 'הבנקאות'), 34),\n",
       " (('הלימות', 'ההון'), 34),\n",
       " (('לחובות', 'מסופקים'), 25),\n",
       " (('במערכת', 'הבנקאית'), 22),\n",
       " (('הקבוצות', 'הבנקאיות'), 19)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(bigram).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print (doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = {}\n",
    "for doc in corpus_tfidf:\n",
    "    for iWord, tf_idf in doc:\n",
    "        if iWord not in topWords:\n",
    "            topWords[iWord] = 0\n",
    "\n",
    "        if tf_idf > topWords[iWord]:\n",
    "            topWords[iWord] = tf_idf\n",
    "\n",
    "for i, item in enumerate(sorted(topWords.items(), key=lambda x: x[1], reverse=True), 1):\n",
    "    print(\"%2s: %-13s %s\" % (i, dictionary[item[0]], item[1]))\n",
    "    if i == 6: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first build 2 lists of pos and neg\n",
    "positive_vocab = [\"מצוין\", \"אדיר\"]\n",
    "negative_vocab = [\"גרוע\"]\n",
    "neutral_vocab = [\"בסדר\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    "neutral_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.8\n",
      "Negative: 0.2\n"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import names\n",
    " \n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\n",
    "negative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\n",
    "neutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not','it' ]\n",
    " \n",
    "positive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\n",
    "negative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\n",
    "neutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n",
    " \n",
    "train_set = negative_features + positive_features + neutral_features\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set) \n",
    " \n",
    "# Predict\n",
    "neg = 0\n",
    "pos = 0\n",
    "sentence = \"Awesome film, I liked it\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "for word in words:\n",
    "    classResult = classifier.classify( word_feats(word))\n",
    "    if classResult == 'neg':\n",
    "        neg = neg + 1\n",
    "    if classResult == 'pos':\n",
    "        pos = pos + 1\n",
    " \n",
    "print('Positive: ' + str(float(pos)/len(words)))\n",
    "print('Negative: ' + str(float(neg)/len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "language_code = \"he\"\n",
    "inp = language_code+\"wiki-latest-pages-articles.xml.bz2\"\n",
    "outp = \"wiki.{}.text\".format(language_code)\n",
    "i = 0\n",
    "\n",
    "print(\"Starting to create wiki corpus\")\n",
    "output = open(outp, 'w')\n",
    "space = \" \"\n",
    "wiki = WikiCorpus(inp, lemmatize=False, dictionary={})\n",
    "for text in wiki.get_texts():\n",
    "  article = space.join([t.decode(\"utf-8\") for t in text])\n",
    "\n",
    "  output.write(article + \"\\n\")\n",
    "  i = i + 1\n",
    "  if (i % 1000 == 0):\n",
    "    print(\"Saved \" + str(i) + \" articles\")\n",
    "\n",
    "output.close()\n",
    "print(\"Finished - Saved \" + str(i) + \" articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
